---
title: "Exclusions"
author: "Mike Frank"
date: "2022-12-15"
output: html_document
---

Goal: consider consequences for reliability of exclusion decisions related to data quantity and behavior.

Need to analyze both RT and accuracy. 

```{r}
source(here::here("helper/common.R"))
```
Let's start with accuracy

# Accuracy

There's a lot of missing data and a lot of "zoners" (kids who look only at one side). Zoners are not just missing data kids, they look in one direction for the entire trial. 

```{r}
d_summary <- d_trial |>
  group_by(dataset_name, trial_id, dataset_id, subject_id, administration_id, 
           target_label) |>
  summarise(total_prop_target_looking = mean(correct, na.rm=TRUE),
            pre_prop_target_looking = mean(correct[t_norm < 300], na.rm=TRUE),
            prop_data = mean(!is.na(correct))) 
```

## Trial-wise

Plot total proportion target looking.

```{r}
ggplot(d_summary,
       aes(x = total_prop_target_looking)) + 
  geom_histogram()
```

In the pre-onset period. 

```{r}
ggplot(d_summary,
       aes(x = pre_prop_target_looking)) + 
  geom_histogram()
```

Plot total proportion of data

```{r}
ggplot(d_summary,
       aes(x = prop_data)) + 
  geom_histogram()
```

Break this down by dataset. 

```{r}
ggplot(d_summary,
       aes(x = prop_data)) + 
  geom_histogram() + 
  facet_wrap(~dataset_name, scales = "free_y")
```

Some datasets have already been filtered for missing data. Others have not. 

## Kid-wise

We can also look at the proportion of data per kid. 

```{r}

d_bykid_summary <- d_summary |>
  group_by(dataset_name, administration_id) |>
  summarise(total_prop_target_looking = 
              mean(total_prop_target_looking, na.rm=TRUE), 
            n_trials = length(unique(trial_id[prop_data > 0])),
            prop_data = mean(prop_data, na.rm=TRUE))
```

```{r}
ggplot(d_bykid_summary, aes(x = prop_data)) + 
  geom_histogram()
```

Looking at targets. 

```{r}
ggplot(d_bykid_summary, aes(x = total_prop_target_looking)) + 
  geom_histogram()
```
```{r}
ggplot(d_bykid_summary, aes(x = total_prop_target_looking, 
                            y = n_trials, col = dataset_name)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  ylab("# trials with any data") + 
  ylab("Avg prop looking at target")
```


# Exclusions 

What are things we could exclude on?

trial-wise:
* prop data in the trial
* only looking at one side in the trial (trial-zoning)
* pre-target word zoning vs. post-target word zoning

subject-wise:
* prop of trials with any data
* prop of "useable trials" (passed the trial-wise exclusions)
* side bias across trials - looking at one side more across trials even if not zoning
* zoning across many trials (kid-zoning) 

Let's find out if any of these increase reliability. 

Our approach: we are going to compute ICCs with different subsamples of the data and see what happens to ICC.

## Simulation

Set up our measures for the simulation. 

### Trial-wise

First stimulation is going to exclude only trial-wise and set a long, constant window across exclusions. 

```{r}
d_sim <- d_trial |>
  group_by(dataset_name, trial_id, dataset_id, subject_id, administration_id, 
           target_label) |>
  summarise(total_target_prop = mean(correct, na.rm=TRUE),
            prop_data = mean(!is.na(correct)), 
            accuracy = mean(correct[t_norm >= 300 & t_norm < 4000], 
                            na.rm=TRUE), 
            pre_looking = mean(correct[t_norm < 300], na.rm=TRUE)) 
```

Main ICC simulation function. 

```{r}
icc_trial_exclusion_sim <- function (zoners_included = "none", 
                                     exclude_less_than, object) 
{
  df <- d_sim |>
    filter(prop_data > exclude_less_than)
  
  # drop zoners
  if (zoners_included == "none") { 
    df <- filter(df, total_target_prop > 0, total_target_prop < 1) 
  } else if (zoners_included == "no pre") {
     df <- filter(df, pre_looking > 0, pre_looking < 1) 
  }
  
  # compute ICCs
  df |> 
    group_by(dataset_name) |> 
    nest() |>
    mutate(icc = unlist(map(data, ~get_icc(., "accuracy", object)))) |>
    select(-data) |>
    unnest(cols = c()) 
}

```

Set parameters and simulate.

```{r}
excl1_params <- expand_grid(zoners_included = c("none","no pre","all"), 
                           exclude_less_than = seq(0,1,.1),
                           object = c("stimulus", "administration"))

# multidyplr 
cluster <- new_cluster(14) 
cluster_library(cluster, "tidyverse")
cluster_library(cluster, "agreement")
cluster_copy(cluster, "icc_trial_exclusion_sim")
cluster_copy(cluster, "get_icc")
cluster_copy(cluster, "d_sim")
  
tic()
excl1 <- excl1_params |> 
  partition(cluster) |>
  mutate(icc = pmap(list(zoners_included, exclude_less_than, object), 
                    icc_trial_exclusion_sim)) |>
  collect() |>
  unnest(col = icc)
toc()

save(file = "cached_intermediates/5_exclusions1.Rds", excl1)
```

Plot resulting ICCs.

```{r}
load("cached_intermediates/5_exclusions1.Rds")

ggplot(excl1,
       aes(x = exclude_less_than, y = icc, 
           group = interaction(dataset_name, zoners_included), 
           col = zoners_included)) + 
  geom_jitter(alpha = .3, width = .03) + 
  geom_line(alpha = .3) + 
  geom_smooth(aes(group = zoners_included), method = "lm") + 
  facet_grid(.~object) + 
  ylab("ICC") + 
  xlab("Include trials with more than")

```
Let's start breaking this down by dataset and looking at the amount of data that is actually being excluded...


```{r}
data_loss_trial_exclusion_sim <- function (zoners_included = "none", 
                                     exclude_less_than = .5) 
{
  ns <- d_sim |>
    group_by(dataset_name, administration_id, trial_id) |>
    count() |>
    group_by(dataset_name) |>
    count()
  
  df <- d_sim |>
    filter(prop_data > exclude_less_than)
  
  # drop zoners
  if (zoners_included == "none") { 
    df <- filter(df, total_target_prop > 0, total_target_prop < 1) 
  } else if (zoners_included == "no pre") {
     df <- filter(df, pre_looking > 0, pre_looking < 1) 
  }
  
  data_loss <- df |>
    group_by(dataset_name, administration_id, trial_id) |>
    count() |>
    group_by(dataset_name) |>
    count() |>
    left_join(rename(ns, n_original = n)) |>
    mutate(prop_trials_retained = n / n_original)
  
  return(data_loss)
}

excl1_loss <- excl1_params |>
  mutate(data_loss = pmap(list(zoners_included, exclude_less_than),
                    data_loss_trial_exclusion_sim)) |>
  unnest(col = data_loss)
```

```{r}

# data_loss_trial_exclusion_sim()

excl1 <- left_join(excl1, excl1_loss)
```

Now plot by dataset loss. Start with administration ICC.

```{r}
ggplot(filter(excl1, object == "administration"),
       aes(x = exclude_less_than, y = icc, 
           group = interaction(dataset_name, zoners_included), 
           col = zoners_included)) + 
  geom_point(aes(size=prop_trials_retained), alpha = .4) + 
  scale_size_area(max_size = 3) +
  geom_line() + 
  # geom_smooth(aes(group = zoners_included), method = "lm") + 
  facet_wrap(~dataset_name) + 
  ylab("ICC") + 
  xlab("Include trials with more than")

```

Let's try plotting this across data loss.

```{r}
ggplot(filter(excl1, object == "administration"),
       aes(x = prop_trials_retained, y = icc, 
           col = zoners_included)) + 
  geom_point() + 
  scale_size_area(max_size = 3) +
  facet_wrap(~zoners_included) + 
  geom_smooth(aes(group = zoners_included), method = "lm") +
  ylab("ICC") + 
  xlab("Prop trials retained")

```
Interpreting this plot:

* if you leave in all zoners, excluding by missing data can lead to increases in subject-wise reliability, but only when you exclude a lot of data....
* if you exclude only full zoners (entire trial), you don't get much benefit.
* excluding pre-zoners does seem to yield a little benefit, and interestingly, no additional benefit for excluding missing data after that. 

Let's look at this dataset by dataset. 

```{r}
ggplot(filter(excl1, object == "administration"),
       aes(x = prop_trials_retained, y = icc, 
           col = zoners_included)) + 
  geom_point() + 
  scale_size_area(max_size = 3) +
  facet_wrap(~dataset_name) + 
  geom_smooth(aes(group = zoners_included), method = "lm") +
  ylab("ICC") + 
  xlab("Prop trials retained")

```


Next steps: 
* look at length of pre- period to understand relation with pre-zoner exclusion.
* potentially snip long pre-period experiments to simulate pre-zoning in short pre-periods - try to distinguish pre-zoning vs. data distribution
* check on target zoning vs. distractor zoning? but maybe unfair to exclude only distractor zoners because it will artificially boost accuracy. 
* check on CDI validity for swingley-aslin - also this paradigm may require checking both alternatives because you actually need to see both to figure out the right answer in a mispronunciation paradigm. 